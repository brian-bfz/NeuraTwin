data_file: "PhysTwin/generated_data/sampled_with_12_edges.h5"
planner_type: "MPPI"           # Options: "MPPI", "GD", "MPPI_GD"
n_sample: 100                  # Number of action trajectories to sample
n_look_ahead: 10               # Number of steps to look ahead
n_update_iter: 5               # Number of optimization iterations
reward_weight: 10.0            # Temperature parameter for MPPI softmax
noise_level: 0.05              # Standard deviation for action noise
  
# Action Space Configuration
action_lower_bound: -0.1       # Lower bound for robot actions (m)
action_upper_bound: 0.1        # Upper bound for robot actions (m)
  
# Optimization Parameters
lr: 0.001                      # Learning rate for gradient descent (if using GD)
verbose: false                 # Enable detailed logging during optimization

# Reward Function Configuration
action_weight: 1.0              # Weight for action penalty in reward function