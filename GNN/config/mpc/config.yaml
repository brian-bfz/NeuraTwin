# Model Predictive Control Configuration
mpc:
  # MPPI Planner Parameters
  planner_type: "MPPI"           # Options: "MPPI", "GD", "MPPI_GD"
  n_sample: 100                  # Number of action trajectories to sample
  n_look_ahead: 10               # Number of steps to look ahead
  n_update_iter: 5               # Number of optimization iterations
  reward_weight: 10.0            # Temperature parameter for MPPI softmax
  noise_level: 0.05              # Standard deviation for action noise
  
  # Action Space Configuration
  action_lower_bound: -0.1       # Lower bound for robot actions (m)
  action_upper_bound: 0.1        # Upper bound for robot actions (m)
  
  # Optimization Parameters
  lr: 0.001                      # Learning rate for gradient descent (if using GD)
  verbose: false                 # Enable detailed logging during optimization

# Reward Function Configuration  
reward:
  # Task-specific reward parameters
  target_position: [0.0, 0.0, 0.0]  # Target position for manipulation tasks
  position_weight: 1.0               # Weight for position-based rewards
  smoothness_weight: 0.1             # Weight for action smoothness penalty
  collision_weight: 10.0             # Weight for collision avoidance
  
  # Distance thresholds
  success_threshold: 0.05            # Distance threshold for task success (m)
  collision_threshold: 0.02          # Distance threshold for collision detection (m)